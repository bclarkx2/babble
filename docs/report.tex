% Class

\documentclass[11pt]{article}


% Info

\title{Babble: Learning Grammar Structure with Generative Adversarial Networks\thanks{\url{https://github.com/bclarkx2/babble}}}
\author{Brian Clark, CWRU}

\date{\today}

% Packages

\usepackage[margin=1in]{geometry}
\usepackage{listings}
\usepackage{float}
\usepackage{blindtext}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{doi}
\usepackage{siunitx}
\usepackage{hyperref}

% Settings

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}
\graphicspath{ {img/} }


% Commands


% Document

\begin{document}

\maketitle

\abstract{Languages have a concept of grammaticality: a sentence is either part of the language or it is not. Many languages can't be fully described by a set of formal rules. A generative adversarial deep learning network is able to learn a set of criteria from examples and produce outputs that satisfy those criteria to an increasing degree. This project proposes an approach using a generative adversarial network to learn grammar structures of languages and generate text samples that match the rules of the language, without explicitly being given the language's formal grammar.}

\newpage
\tableofcontents
\newpage

\section{Introduction}

\subsection{Problem}

The English language is undeniably a complex construct. English grammar is composed of dozens of rules, many of which are only loosely applied. The vocabulary is incredibly vast, and growing everyday. English is consistently rated one of the most difficult world languages for new speakers to learn.

And yet, even young children are capable of producing speech that is widely accepted as "English" -- at least, most of the time. Clearly, the human brain has a well developed capability for internalizing the deep structure of a language like English and generating examples that match that structure.

This naturally leads to the question: can intelligences other than human brains perform a similar feat? The purpose of this project is to produce a deep learning network that, given examples of text from a language, is able to produce text samples that comply with the rules of that language.

\subsection{Grammar}

The first challenge built into this learning problem is the definition of a language.

A approach to language definition is to think of a language as a set of formal rules called a grammar\cite{nltk_book}. Any piece of text satisfying the rules of the grammar can be considered part of the language inquestion. This grammar can be thought of as an operational definition of a language.

This leads to a natural conclusion in the realm of generative networks: a grammar could serve as a loss function for a network. The network would be rewarded for creating a grammatical sentence and penalized for created a non-grammatical sentence.

The limitations with this approach are the same as the limitations for grammars in general. For any sufficiently complex language, it becomes difficult to write a correct and complete grammar describing the language. For example, there is no one extant grammar that is widely regarded to represent the entirety of the English language. It may not even be possible to create such a grammar.

Without the feedback provided by a formal grammar, there is still one source of grammaticality available to us: actual sentences. Ideally, the generative network could use an existing corpus of sentences in a language as the basis for building a model of the language. Using data itself to glean structure rather than a set of predesigned rules is a mainstay of machine learning. This is the approach followed in this project.

\subsection{Generative Adversarial Networks}

A tool explored in this project is a deep learning model known as Generative Adversarial Networks \cite{gen_adv_model}.

The idea behind the generative adversarial approach is to split the network into two subnetworks, each with a distinct role:

\begin{itemize}
    \item \textbf{Discriminator}: The discriminator is trained to recognize the difference between examples that meet certain criteria and examples that do not. For example, the discriminator would be able to tell if a given sentence was grammatical or not in a language.
    \item \textbf{Generator}: This subnetwork is responsible for transforming random noise inputs into samples that meet the criteria being judged for by the discriminator. In this example, the generator would attempt to create phrases out of random noise that follow the rules of a language.
\end{itemize}

The insight behind this model, however, is the interplay between the two subsystem. The outputs from the generator are fed into the discriminator and labelled as negative examples. This creates a feedback loop where the discriminator gets ever better at distinguishing fake sentences from real sentences which pushes the generator to produce more realistic sentences.

\section{Background}

Generative adversarial networks are certainly in use to solve other problems in the machine learning world. Atienza uses a generative adversarial model on the MNIST data set in an attempt to computer-generate handwritten images that look similar to those in the MNIST data set \cite{gen_adv_example}. Some ideas from this implementation proved useful during this project.

Furthermore, the concept of using a generative adversarial network in the field of text generation is already sparking academic interest. Zhang, Gan, and Carin demonstrate an approach that uses a convolutional neural network (CNN) as a discriminator and a long short term memory network (LSTM) as a generator \cite{gan}. With this setup, they are able to generate realistic English text.

\section{Approach}

To accomplish the task of generating realistic text, I took the following steps.

\begin{enumerate}
    \item Established network model
    \item Created grammar representation
    \item Developed network implementations
    \item Devised several learning 'modes'
    \item Tuned specific network hyperparameters
\end{enumerate}

\noindent The following sections describe each of these steps.

\subsection{Network Model}

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.5]{img/network_model.pdf}
    \caption{Network Model}
    \label{fig:net_model}
\end{figure}

The generator takes as input a vector of random noise data, generated using the \texttt{numpy.random} module. It produces a vector of index values, where each index maps to a given word from the language's dictionary. In essence, therefore, the ouput from the generator is a sentence, as each word index in the output vector can be mapped to a word and the list of words forms a sentence.

The discriminator's input, on the other hand, is a vector of word indices. These can be drawn either from the corpus of actual sentences or from the generator itself. The discriminator is then responsible for generating a boolean classification value representing whether or not the given sentence is grammatical.

\subsection{Grammar}

As mentioned above, we cannot necessarily write a grammar that encapsulates all the complexity of the entire English language. However, it is straightforward to create a context-free grammar that succinctly describes a subset of all possible English sentences.

\subsubsection{Interface}

As an intermediate step to training a network to produce English sentences, this project provides an interface for defining smaller grammars and generating sentences that use them.

Figure \ref{fig:grammar} shows the interface developed for this purpose.
\begin{figure}[ht]
    \centering
    \includegraphics[trim={0 0 0 2.45cm},scale=0.7,clip]{img/grammar.pdf}
    \caption{Grammar interface}
    \label{fig:grammar}
\end{figure}

\subsubsection{Tools}

To actually represent the rules of this grammar, the \texttt{nltk} (Natural Language Toolkit) Python package provides several useful tools \cite{nltk_book}. Namely, they provide a format for describing the rules of a grammar (as detailed in the next section) and several algorithms for parsing said grammars. This project makes use of the recursive descent parser.

\subsubsection{Implementations}

Using the interface described above, this project implements the following grammars:\\

\noindent \textbf{SimpleGrammar}

\noindent This grammar represents a very small subset of English language sentences. In this language, all sentences take the form ``subject verb object'', and the total vocabulary is just 20 words. Please see Figure \ref{fig:simple_grammar} for a formal description of this context-free grammar.

\begin{figure}[ht]
    \centering
    \begin{lstlisting}
        S -> N V O
        N -> 'John'
        N -> 'Mary'
        N -> 'Alex'
        N -> 'Jack'
        N -> 'Nate'
        N -> 'Wallace'
        N -> 'Kumail'
        N -> 'Mahmoud'
        N -> 'Pierre'
        N -> 'Katrina'
        V -> 'saw'
        V -> 'found'
        V -> 'threw'
        V -> 'lost'
        O -> 'pie'
        O -> 'cheese'
        O -> 'milk'
        O -> 'grapes'
        O -> 'apples'
        O -> 'pears'
    \end{lstlisting}
    \caption{SimpleGrammar representation}
    \label{fig:simple_grammar}
\end{figure}

\subsection{Network Implementation}

The deep learning networks in Babble are implemented using the \texttt{keras} Python library, and designed to run with TensorFlow as the backend tensor manipulation framework \cite{chollet2015keras} \cite{tensorflow}.

The following sections describe some of the specific network implementations used, and the reasoning behind them. \\

\subsubsection{Generators}

Source code for the generators may be found in \texttt{generator.py} \\

\noindent \textbf{dense\_only\_gen}

This generator is composed of only dense layers. It follows the network layout described in Figure \ref{fig:dense_only_gen}.

The reasoning behind this network is to provide a basic generator model with no extra frills. Throw a bunch of neurons at the problem, and see what fits.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.65]{img/dense_only_gen}
    \caption{dense\_only\_gen network layout}
    \label{fig:dense_only_gen}
\end{figure}

\subsubsection{Discriminators}

Source code for the discriminators can be found in \texttt{babble/discriminator.py} \\

\noindent \textbf{no\_conv\_disc}

The following discriminator is implemented without any convolution layers using the network layout described in Figure \ref{fig:no_conv_disc}

\begin{figure}[ht]
    \centering
    \includegraphics{no_conv_disc.pdf}
    \caption{no\_conv\_disc network layout}
    \label{fig:no_conv_disc}
\end{figure}

\subsection{Learning Modes}

With the basic network structure defined, the next step is to exercise the learning system in different ways to see interesting results. Here are the three different modes employed:

\subsubsection{Grammaticality Training}

In this mode, only the discriminator is trained. Here are the steps involved:
\begin{enumerate}
    \item A Grammar object (Figure \ref{fig:grammar}) is generated
    \item That Grammar object is used to generate many examples of valid sentences and many examples of invalid sentences.
    \item The discriminator is trained on these examples.
    \begin{itemize}
        \item Using \texttt{keras.layers.Sequential.fit}
        \item RMSProp optimization
        \item Binary cross entropy loss function
        \begin{itemize}
            \item Learning rate: \num{2e-3}
            \item Decay: \num{6e-8}
        \end{itemize}
        \item Batches of size 32
        \item Validation split: 10\%
    \end{itemize}
\end{enumerate}

\subsubsection{Oracle Training}

This learning mode is designed to train the generator alone.

Steps:
\begin{enumerate}
    \item Train the discriminator using the Grammaticality Training mode described above.
    \item Fix the weights in the discriminator.
    \item Feed the combined model (\ref{fig:net_model}) with random noise and label it positive (i.e. that it corresponds to a valid sentence). The trained discriminator will penalize the generator if it doesn't produce a valid sentence.
    \begin{itemize}
        \item Binary cross entropy loss function
        \begin{itemize}
            \item Learning rate: \num{1e-3}
            \item Decay: \num{0.0}
        \end{itemize}
        \item Batches of size 1000
    \end{itemize}
    \item Repeat until the loss is reliably low
\end{enumerate}

\subsubsection{Adversarial Training}

The adversarial training mode is the main training mode for this project.

\begin{enumerate}
    \item Select some valid sentences from the corpus. Label them as positive.
    \item Exercise the generator several times and label the results as negative.
    \item Use those two sets of examples to train the discriminator, using the same parameters as in the Grammaticality Training above.
    \item Feed the combined model (\ref{fig:net_model}) with random noise and label it positive (i.e. that it corresponds to a valid sentence). This will induce the generator to improve itself using the recently trained discriminator.
\end{enumerate}

\section{Results}

In this section, I ran the network in the three modes described in the Experiments section. All of the following data was produced with a GeForce GTX 860M graphics card with 1.95GiB of memory.

\subsection{Grammaticality Training}

After running for 250 iterations with a batch size of 100, the discriminator achieved the metrics on the entire dataset of SimpleGrammar sentences seen in Table \ref{tab:disc_results}.

\begin{table}[ht]
    \centering
    \begin{tabular}{c|c}
        \hline
        Cross Entropy & 0.047 \\
        Accuracy & 985 \\
    \end{tabular}
    \caption{Discriminator Results}
    \label{tab:disc_results}
\end{table}

\subsection{Oracle Training}

The generator, running with a pre-trained discriminator for 1000 iterations was able to produce the series of sentences found in Table \ref{tab:oracle_results}. At each iteration shown in the table, the generator was fed a random input and asked to form a sentence from it.


\begin{table}[ht]
    \centering
    \begin{tabular}{c|c|c}
        Iteration & Loss & Sentence \\
        \hline
        0 & 16.118 & mahmoud wallace pierre \\
        100 & 16.118 & katrina kumail pierre \\
        200 & \num{7.175e-4} & kumail lost milk \\
        300 & \num{2.096e-4} & wallace found milk \\
        400 & \num{1.543e-4} & wallace found pears \\
        500 & \num{7.327e-5} & wallace found pears \\
        600 & \num{6.395e-5} & wallace found pears \\
        700 & \num{6.927e-5} & wallace found pears \\
        800 & \num{1.471e-4} & wallace found pears \\
        900 & \num{1.437e-4} & wallace found pears \\
    \end{tabular}
    \caption{Oracle Training Results}
    \label{tab:oracle_results}
\end{table}

\subsection{Adversarial Training}

Finally, exercising the full model for 2000 iterations with a batch size of 10 produced the results shown in Table \ref{tab:adversarial_results}. At each iteration shown in the table below, the generator was given a random input and instructed to produce a sentence.

\begin{table}[ht]
    \centering
    \begin{tabular}{c|c}
        Iteration & Sentence \\
        \hline
        0 & mahmoud wallace pierre \\
        100 & nate kumail lost \\
        200 & alex pierre pie \\
        300 & alex saw cheese \\
        400 & wallace saw lost \\
        500 & wallace saw grapes \\
        600 & wallace threw apples \\
        700 & mahmoud pie pears \\
        800 & wallace pie pears \\
        900 & wallace threw pears \\
        1000 & mahmoud cheese pears \\
        1100 & kumail pie pears \\
        1200 & kumail lost pears \\
        1300 & wallace threw pears \\
        1400 & wallace found apples \\
        1500 & kumail found grapes \\
        1600 & nate saw milk \\
        1700 & nate saw cheese \\
        1800 & kumail found grapes \\
        1900 & mahmoud threw apples \\
    \end{tabular}
    \caption{Adversarial Training Results}
    \label{tab:adversarial_results}
\end{table}


\section{Discussion}

The results from training the discriminative network alone are very promising. Clearly, a simple network is sufficient to almost completely learn to identify whether a sentence satisfies the rules of SimpleGrammar.

Similarly, the oracular training also yielded positive results. After only 1000 iterations with the oracle, the generator was able to consistently produce a grammatical sentence. The most interesting result, however, is that this training method always produces the same sentence. It seems plausible that the lack of flexibility in the oracle plays a role in this phenomenon. This would be a great topic for further exploration.

The adversarial training method was in fact to produce a variety of sentences that satisfied the SimpleGrammar rules. Initially, it produced sentences that were garbage, such as "nate kumail lost." However, as the generator and discriminator played off each other, they were able to flesh out the structure of the language and began producing mostly grammatical sentences.

\section{Conclusions}

In the end, it seems that the approach of using a generative adversarial network to learn language structures has promise. Two very simple networks working in competition were able to get decent results in identifying sentences belonging to a simple grammar and in generating sentences according to that grammar.

Future avenues of study could help illuminate some of these possibilities. For example:

\begin{itemize}
    \item Expanding to use more complicated grammars than SimpleGrammar -- especially ones that allow sentences of variable lengths.
    \item Using the techniques developed here on English sentences.
    \item Doing additional, formal parameter tuning on the network structures described here.
    \item Explore additional network structures for both the generator and the discriminator.
\end{itemize}

\newpage
\bibliography{sources}
\bibliographystyle{plain}

\end{document}
